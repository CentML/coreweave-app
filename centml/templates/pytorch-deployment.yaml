apiVersion: apps/v1
kind: Deployment
metadata:
  name: centml
  labels:
    app: centml
spec:
  strategy:
    type: Recreate
  # Replicas controls the number of instances of the Pod to maintain running at all times
  replicas: 1
  selector:
    matchLabels:
      app: centml
  template:
    metadata:
      labels:
        app: centml
    spec:
      containers:
        - name: pytorch
          image: tverous/pytorch-notebook:latest
          ports:
            - name: notebook
              containerPort: 8888
              protocol: TCP

          readinessProbe:
            tcpSocket:
              port: notebook
            initialDelaySeconds: 5
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /
              port: notebook
            initialDelaySeconds: 15
            periodSeconds: 15
            failureThreshold: 3
            timeoutSeconds: 10
          # volumeMounts:
          #   - name: storage
          #     mountPath: /pytorch/notebooks

          # resources:
          #   requests:
          #     cpu: 8 # The CPU unit is mili-cores. 500m is 0.5 cores
          #     memory: 48Gi
          #   limits:
          #     # GPUs can only be allocated as a limit, which both reserves and limits the number of GPUs the Pod will have access to
          #     # Making individual Pods resource light is advantageous for bin-packing. In the case of Jupyter, we stick to two GPUs for
          #     # demonstration purposes
          #     nvidia.com/gpu: 1
      # affinity:
      #   nodeAffinity:
      #     # This will REQUIRE the Pod to be run on a system with a Pascal GPU
      #     requiredDuringSchedulingIgnoredDuringExecution:
      #       nodeSelectorTerms:
      #       - matchExpressions:
      #         - key: gpu.nvidia.com/class
      #           operator: In
      #           values:
      #             - NV_Pascal
      #         - key: topology.kubernetes.io/region
      #           operator: In
      #           values:
      #             - ORD1

      #     # As ML testing doesn't require a lot of network throughput, we try to play nice and only schedule
      #     # the Pod on systems with only 1G network connections. We also desire decent CPUs. This is a preference, not a requirement.
      #     # If systems with i5 / i9 / Xeon CPUs and/or 1G ethernet are not available to fulfill the requested resources, the Pods
      #     # will be scheduled on higher end systems.
      #     preferredDuringSchedulingIgnoredDuringExecution:
      #       - weight: 10
      #         preference:
      #           matchExpressions:
      #           - key: cpu.atlantic.cloud/family
      #             operator: In
      #             values:
      #               - i7
      #               - i5
      #               - i9
      #               - xeon
      #       - weight: 10
      #         preference:
      #           matchExpressions:
      #           - key: ethernet.atlantic.cloud/speed
      #             operator: In
      #             values:
      #               - 1G
      # Node affinity can be used to require / prefer the Pods to be scheduled on a node with a specific hardware type
      # No affinity allows scheduling on all hardware types that can fulfill the resource request.
      # In this example, without affinity, any NVIDIA GPU would be allowed to run the Pod.
      # Read more about affinity at: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
      # volumes:
      #   - name: storage
      #     persistentVolumeClaim:
      #       claimName: jupyter-pv-claim
      # restartPolicy: Always